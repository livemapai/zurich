"""
LLM-powered creative variation generator for styled tiles.

The LLM has full creative control over APPEARANCE while geometry stays fixed.
This includes:
- Per-object color variations (each tree/building unique)
- Global style parameters (lighting, atmosphere, mood)
- Creative interpretations of style prompts

What the LLM controls:
âœ… Building colors (walls, roofs, emission)
âœ… Tree colors (foliage per-species, bark)
âœ… Lighting (sun position, color, intensity)
âœ… Atmosphere (fog, glow, reflections)
âœ… Ground/terrain colors
âœ… Sky colors
âœ… Post-processing (saturation, contrast, warmth)
âœ… Special effects (neon, snow, rain)

What stays FIXED (from your data):
ðŸ”’ Building positions (65k footprints)
ðŸ”’ Building heights
ðŸ”’ Tree positions (80k trees)
ðŸ”’ Tree sizes (crown diameter, height)
ðŸ”’ Street layouts
ðŸ”’ Terrain elevation

Usage:
    generator = LLMVariationGenerator()

    # Creative style from text prompt
    params = generator.generate_style("cyberpunk night rain")

    # Per-object variations
    tree_colors = generator.generate_tree_variations(trees, "autumn")
    building_colors = generator.generate_building_variations(buildings, "night")
"""

import json
import hashlib
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import os

# Type alias
RGB = Tuple[float, float, float]


@dataclass
class LLMStyleOutput:
    """Complete style configuration generated by LLM."""

    # Building materials
    building_wall_color: RGB = (0.85, 0.82, 0.78)
    building_roof_color: RGB = (0.45, 0.38, 0.32)
    building_emission: float = 0.0  # 0=none, 1+=glowing
    building_roughness: float = 0.8
    window_emission_color: Optional[RGB] = None  # For night scenes

    # Tree materials
    tree_foliage_color: RGB = (0.28, 0.48, 0.22)
    tree_trunk_color: RGB = (0.35, 0.25, 0.18)
    tree_color_variation: float = 0.1  # How much trees differ from each other

    # Ground/terrain
    ground_color: RGB = (0.35, 0.50, 0.25)
    terrain_color: RGB = (0.45, 0.42, 0.38)
    street_color: RGB = (0.35, 0.35, 0.38)
    street_wetness: float = 0.0  # 0-1, reflectivity

    # Lighting
    sun_azimuth: float = 225.0  # Degrees from north
    sun_altitude: float = 35.0  # Degrees above horizon
    sun_color: RGB = (1.0, 0.98, 0.95)
    sun_strength: float = 3.0
    ambient_color: RGB = (0.7, 0.8, 1.0)
    ambient_strength: float = 0.3

    # Atmosphere
    sky_color: RGB = (0.7, 0.8, 1.0)
    fog_color: Optional[RGB] = None
    fog_density: float = 0.0

    # Post-processing
    saturation: float = 1.0
    contrast: float = 1.0
    temperature_shift: float = 0.0  # -1 cold to +1 warm
    brightness: float = 1.0

    # Special effects
    snow_coverage: float = 0.0
    rain_intensity: float = 0.0
    neon_glow: bool = False
    neon_colors: Optional[List[RGB]] = None  # For cyberpunk

    # Metadata
    style_prompt: str = ""
    seed: Optional[int] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to JSON-serializable dictionary."""
        d = {
            "building_wall_color": list(self.building_wall_color),
            "building_roof_color": list(self.building_roof_color),
            "building_emission": self.building_emission,
            "building_roughness": self.building_roughness,
            "tree_foliage_color": list(self.tree_foliage_color),
            "tree_trunk_color": list(self.tree_trunk_color),
            "tree_color_variation": self.tree_color_variation,
            "ground_color": list(self.ground_color),
            "terrain_color": list(self.terrain_color),
            "street_color": list(self.street_color),
            "street_wetness": self.street_wetness,
            "sun_azimuth": self.sun_azimuth,
            "sun_altitude": self.sun_altitude,
            "sun_color": list(self.sun_color),
            "sun_strength": self.sun_strength,
            "ambient_color": list(self.ambient_color),
            "ambient_strength": self.ambient_strength,
            "sky_color": list(self.sky_color),
            "fog_density": self.fog_density,
            "saturation": self.saturation,
            "contrast": self.contrast,
            "temperature_shift": self.temperature_shift,
            "brightness": self.brightness,
            "snow_coverage": self.snow_coverage,
            "rain_intensity": self.rain_intensity,
            "neon_glow": self.neon_glow,
            "style_prompt": self.style_prompt,
            "seed": self.seed,
        }
        if self.window_emission_color:
            d["window_emission_color"] = list(self.window_emission_color)
        if self.fog_color:
            d["fog_color"] = list(self.fog_color)
        if self.neon_colors:
            d["neon_colors"] = [list(c) for c in self.neon_colors]
        return d


@dataclass
class PerObjectVariation:
    """Per-object color variation from LLM."""
    object_id: str
    hue_shift: float = 0.0  # -30 to +30 degrees
    saturation_mult: float = 1.0  # 0.7 to 1.3
    brightness_mult: float = 1.0  # 0.85 to 1.15
    custom_color: Optional[RGB] = None  # Override if specified


class LLMVariationGenerator:
    """Generates creative variations using LLM.

    The LLM can adjust anything visual while geometry stays fixed.
    """

    def __init__(
        self,
        model: str = "claude-3-5-sonnet-20241022",
        cache_dir: Optional[Path] = None,
    ):
        """Initialize the generator.

        Args:
            model: LLM model to use
            cache_dir: Directory to cache LLM responses for reproducibility
        """
        self.model = model
        self.cache_dir = cache_dir or Path(".llm_cache")
        self.cache_dir.mkdir(exist_ok=True)

    def generate_style(
        self,
        prompt: str,
        seed: Optional[int] = None,
        use_cache: bool = True,
    ) -> LLMStyleOutput:
        """Generate complete style parameters from a text prompt.

        The LLM has full creative freedom to interpret the prompt and
        generate appropriate colors, lighting, atmosphere, etc.

        Args:
            prompt: Creative style description (e.g., "cyberpunk rain night")
            seed: Random seed for reproducibility
            use_cache: Whether to use cached responses

        Returns:
            LLMStyleOutput with all visual parameters
        """
        # Check cache first
        cache_key = self._cache_key(f"style_{prompt}_{seed}")
        if use_cache and self._has_cache(cache_key):
            return self._load_cache(cache_key)

        # Build the LLM prompt
        system_prompt = """You are a creative art director for 3D map tile rendering.
You generate visual style parameters for city scenes. Be creative and dramatic!

Your output controls everything VISUAL:
- Building colors (walls, roofs, windows)
- Tree/vegetation colors
- Lighting (sun position, color, intensity)
- Atmosphere (fog, sky color)
- Ground/street appearance
- Post-processing (saturation, contrast, warmth)
- Special effects (snow, rain, neon glow)

The GEOMETRY is fixed - you're styling existing 3D buildings and trees, not generating new ones.

Return a JSON object with these fields (all colors as [R,G,B] 0.0-1.0):
{
  "building_wall_color": [R, G, B],
  "building_roof_color": [R, G, B],
  "building_emission": float (0=normal, 1+=glowing),
  "building_roughness": float (0=shiny, 1=matte),
  "window_emission_color": [R, G, B] or null,
  "tree_foliage_color": [R, G, B],
  "tree_trunk_color": [R, G, B],
  "tree_color_variation": float (0-0.3, how much trees differ),
  "ground_color": [R, G, B],
  "terrain_color": [R, G, B],
  "street_color": [R, G, B],
  "street_wetness": float (0-1, reflectivity),
  "sun_azimuth": float (0=N, 90=E, 180=S, 270=W),
  "sun_altitude": float (-10 to 90, below horizon for night),
  "sun_color": [R, G, B],
  "sun_strength": float (0.1 to 5),
  "ambient_color": [R, G, B],
  "ambient_strength": float (0-1),
  "sky_color": [R, G, B],
  "fog_color": [R, G, B] or null,
  "fog_density": float (0-0.5),
  "saturation": float (0.5-1.5),
  "contrast": float (0.7-1.3),
  "temperature_shift": float (-1 cold to +1 warm),
  "brightness": float (0.5-1.5),
  "snow_coverage": float (0-1),
  "rain_intensity": float (0-1),
  "neon_glow": boolean,
  "neon_colors": [[R,G,B], ...] if neon_glow else null
}

Be creative! Match the mood of the prompt."""

        user_prompt = f"""Generate style parameters for: "{prompt}"

Think about:
- What time of day fits this mood?
- What lighting creates this atmosphere?
- What colors evoke this feeling?
- Should there be weather effects?

Return ONLY the JSON object, no explanation."""

        # Call LLM (fallback to preset if no API key)
        try:
            result_json = self._call_llm(system_prompt, user_prompt, seed)
            style = self._parse_style_response(result_json, prompt, seed)
        except Exception as e:
            # Fallback to intelligent preset matching
            print(f"LLM call failed ({e}), using preset fallback")
            style = self._fallback_style(prompt, seed)

        # Cache the result
        if use_cache:
            self._save_cache(cache_key, style)

        return style

    def generate_tree_variations(
        self,
        trees: List[Dict],
        season: str = "summer",
        style_prompt: Optional[str] = None,
        seed: Optional[int] = None,
    ) -> Dict[str, PerObjectVariation]:
        """Generate per-tree color variations.

        Each tree gets a unique but natural-looking color within species bounds.

        Args:
            trees: List of tree data dicts with 'id' and 'species'
            season: spring/summer/autumn/winter
            style_prompt: Optional creative prompt
            seed: Random seed

        Returns:
            Dict mapping tree_id to PerObjectVariation
        """
        # Group trees by species for efficient batching
        by_species: Dict[str, List[Dict]] = {}
        for tree in trees:
            species = tree.get("species", "default")
            genus = species.split()[0] if species else "default"
            if genus not in by_species:
                by_species[genus] = []
            by_species[genus].append(tree)

        # Check cache
        cache_key = self._cache_key(f"trees_{season}_{style_prompt}_{seed}_{len(trees)}")
        if self._has_cache(cache_key):
            return self._load_cache(cache_key)

        # Generate variations per species
        variations: Dict[str, PerObjectVariation] = {}

        try:
            system_prompt = f"""You generate natural color variations for trees.
Season: {season}
Each tree should look slightly different but natural.

For deciduous trees in {season}:
- spring: fresh light greens, some with blossoms
- summer: rich greens, slightly varied
- autumn: reds, oranges, yellows (maples redder, oaks browner)
- winter: bare brown branches

For evergreens (pine, spruce, fir): always green, slight variation.

Return JSON array of variations:
[{{"id": "tree_123", "hue_shift": -5, "saturation_mult": 0.95, "brightness_mult": 1.02}}]

Keep variations subtle but noticeable (hue Â±15Â°, sat/bright Â±15%)."""

            for genus, genus_trees in by_species.items():
                user_prompt = f"""Generate variations for {len(genus_trees)} {genus} trees.
Tree IDs: {[t.get('id', i) for i, t in enumerate(genus_trees[:50])]}
{"Style hint: " + style_prompt if style_prompt else ""}

Return JSON array with natural variation."""

                result = self._call_llm(system_prompt, user_prompt, seed)
                genus_variations = json.loads(result)

                for var in genus_variations:
                    tree_id = str(var.get("id", ""))
                    variations[tree_id] = PerObjectVariation(
                        object_id=tree_id,
                        hue_shift=var.get("hue_shift", 0),
                        saturation_mult=var.get("saturation_mult", 1.0),
                        brightness_mult=var.get("brightness_mult", 1.0),
                    )

        except Exception as e:
            print(f"LLM tree variation failed ({e}), using random fallback")
            variations = self._fallback_tree_variations(trees, seed)

        self._save_cache(cache_key, variations)
        return variations

    def generate_building_variations(
        self,
        buildings: List[Dict],
        style_prompt: Optional[str] = None,
        seed: Optional[int] = None,
    ) -> Dict[str, PerObjectVariation]:
        """Generate per-building color variations.

        Each building gets subtle variation within its type bounds.

        Args:
            buildings: List of building data with 'id' and 'type'
            style_prompt: Optional creative prompt
            seed: Random seed

        Returns:
            Dict mapping building_id to PerObjectVariation
        """
        cache_key = self._cache_key(f"buildings_{style_prompt}_{seed}_{len(buildings)}")
        if self._has_cache(cache_key):
            return self._load_cache(cache_key)

        variations: Dict[str, PerObjectVariation] = {}

        try:
            system_prompt = """You generate subtle color variations for buildings.
Each building should be slightly different but maintain neighborhood coherence.

For residential: warm tones, slight variation
For commercial: cooler, more uniform
For industrial: grays, more variation

Keep variations subtle (hue Â±10Â°, sat/bright Â±10%).
{"style_prompt": Style hint if provided}

Return JSON array:
[{"id": "bldg_1", "hue_shift": 3, "saturation_mult": 0.98, "brightness_mult": 1.01}]"""

            # Sample buildings for API efficiency
            sample = buildings[:100]
            user_prompt = f"""Generate variations for {len(sample)} buildings.
Types: {set(b.get('type', 'default') for b in sample)}
{f'Style: {style_prompt}' if style_prompt else ''}"""

            result = self._call_llm(system_prompt, user_prompt, seed)
            building_variations = json.loads(result)

            for var in building_variations:
                bldg_id = str(var.get("id", ""))
                variations[bldg_id] = PerObjectVariation(
                    object_id=bldg_id,
                    hue_shift=var.get("hue_shift", 0),
                    saturation_mult=var.get("saturation_mult", 1.0),
                    brightness_mult=var.get("brightness_mult", 1.0),
                )

        except Exception as e:
            print(f"LLM building variation failed ({e}), using random fallback")
            variations = self._fallback_building_variations(buildings, seed)

        self._save_cache(cache_key, variations)
        return variations

    def _call_llm(
        self,
        system_prompt: str,
        user_prompt: str,
        seed: Optional[int] = None,
    ) -> str:
        """Call the LLM API.

        Tries multiple providers in order: Anthropic, OpenAI, Google.
        """
        # Try Anthropic
        if os.environ.get("ANTHROPIC_API_KEY"):
            return self._call_anthropic(system_prompt, user_prompt, seed)

        # Try OpenAI
        if os.environ.get("OPENAI_API_KEY"):
            return self._call_openai(system_prompt, user_prompt, seed)

        # Try Google
        if os.environ.get("GOOGLE_API_KEY"):
            return self._call_google(system_prompt, user_prompt, seed)

        raise RuntimeError(
            "No LLM API key found. Set one of: "
            "ANTHROPIC_API_KEY, OPENAI_API_KEY, GOOGLE_API_KEY"
        )

    def _call_anthropic(
        self,
        system_prompt: str,
        user_prompt: str,
        seed: Optional[int] = None,
    ) -> str:
        """Call Anthropic Claude API."""
        import anthropic

        client = anthropic.Anthropic()
        response = client.messages.create(
            model=self.model,
            max_tokens=4096,
            system=system_prompt,
            messages=[{"role": "user", "content": user_prompt}],
        )
        return response.content[0].text

    def _call_openai(
        self,
        system_prompt: str,
        user_prompt: str,
        seed: Optional[int] = None,
    ) -> str:
        """Call OpenAI API."""
        import openai

        client = openai.OpenAI()
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            seed=seed,
        )
        return response.choices[0].message.content

    def _call_google(
        self,
        system_prompt: str,
        user_prompt: str,
        seed: Optional[int] = None,
    ) -> str:
        """Call Google Gemini API."""
        import google.generativeai as genai

        genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(f"{system_prompt}\n\n{user_prompt}")
        return response.text

    def _parse_style_response(
        self,
        response: str,
        prompt: str,
        seed: Optional[int],
    ) -> LLMStyleOutput:
        """Parse LLM JSON response into LLMStyleOutput."""
        # Extract JSON from response (might have markdown wrapping)
        if "```json" in response:
            response = response.split("```json")[1].split("```")[0]
        elif "```" in response:
            response = response.split("```")[1].split("```")[0]

        data = json.loads(response.strip())

        return LLMStyleOutput(
            building_wall_color=tuple(data.get("building_wall_color", [0.85, 0.82, 0.78])),
            building_roof_color=tuple(data.get("building_roof_color", [0.45, 0.38, 0.32])),
            building_emission=data.get("building_emission", 0.0),
            building_roughness=data.get("building_roughness", 0.8),
            window_emission_color=tuple(data["window_emission_color"]) if data.get("window_emission_color") else None,
            tree_foliage_color=tuple(data.get("tree_foliage_color", [0.28, 0.48, 0.22])),
            tree_trunk_color=tuple(data.get("tree_trunk_color", [0.35, 0.25, 0.18])),
            tree_color_variation=data.get("tree_color_variation", 0.1),
            ground_color=tuple(data.get("ground_color", [0.35, 0.50, 0.25])),
            terrain_color=tuple(data.get("terrain_color", [0.45, 0.42, 0.38])),
            street_color=tuple(data.get("street_color", [0.35, 0.35, 0.38])),
            street_wetness=data.get("street_wetness", 0.0),
            sun_azimuth=data.get("sun_azimuth", 225.0),
            sun_altitude=data.get("sun_altitude", 35.0),
            sun_color=tuple(data.get("sun_color", [1.0, 0.98, 0.95])),
            sun_strength=data.get("sun_strength", 3.0),
            ambient_color=tuple(data.get("ambient_color", [0.7, 0.8, 1.0])),
            ambient_strength=data.get("ambient_strength", 0.3),
            sky_color=tuple(data.get("sky_color", [0.7, 0.8, 1.0])),
            fog_color=tuple(data["fog_color"]) if data.get("fog_color") else None,
            fog_density=data.get("fog_density", 0.0),
            saturation=data.get("saturation", 1.0),
            contrast=data.get("contrast", 1.0),
            temperature_shift=data.get("temperature_shift", 0.0),
            brightness=data.get("brightness", 1.0),
            snow_coverage=data.get("snow_coverage", 0.0),
            rain_intensity=data.get("rain_intensity", 0.0),
            neon_glow=data.get("neon_glow", False),
            neon_colors=[tuple(c) for c in data["neon_colors"]] if data.get("neon_colors") else None,
            style_prompt=prompt,
            seed=seed,
        )

    def _fallback_style(self, prompt: str, seed: Optional[int]) -> LLMStyleOutput:
        """Generate style from prompt without LLM using keyword matching."""
        prompt_lower = prompt.lower()

        # Base style
        style = LLMStyleOutput(style_prompt=prompt, seed=seed)

        # Keyword matching for style
        if "night" in prompt_lower:
            style.sun_altitude = -10
            style.sun_strength = 0.1
            style.brightness = 0.4
            style.sky_color = (0.05, 0.08, 0.15)
            style.ambient_strength = 0.15

        if "cyberpunk" in prompt_lower or "neon" in prompt_lower:
            style.neon_glow = True
            style.neon_colors = [(1.0, 0.0, 0.8), (0.0, 1.0, 1.0), (1.0, 1.0, 0.0)]
            style.building_emission = 0.3
            style.window_emission_color = (1.0, 0.0, 0.8)
            style.street_wetness = 0.8
            style.saturation = 1.3
            style.sky_color = (0.08, 0.02, 0.12)

        if "winter" in prompt_lower or "snow" in prompt_lower:
            style.snow_coverage = 0.8
            style.sun_warmth = 0.7
            style.temperature_shift = -0.15
            style.brightness = 1.1
            style.tree_foliage_color = (0.15, 0.32, 0.15)
            style.ground_color = (0.90, 0.92, 0.95)

        if "autumn" in prompt_lower or "fall" in prompt_lower:
            style.tree_foliage_color = (0.85, 0.50, 0.20)
            style.sun_warmth = 1.4
            style.temperature_shift = 0.2
            style.saturation = 1.15

        if "rain" in prompt_lower or "wet" in prompt_lower:
            style.rain_intensity = 0.7
            style.street_wetness = 0.8
            style.fog_density = 0.1

        if "fog" in prompt_lower or "mist" in prompt_lower:
            style.fog_density = 0.3
            style.fog_color = (0.8, 0.82, 0.85)
            style.contrast = 0.8

        if "sunset" in prompt_lower or "golden" in prompt_lower:
            style.sun_altitude = 12
            style.sun_color = (1.0, 0.85, 0.65)
            style.sun_warmth = 1.8
            style.temperature_shift = 0.35
            style.sky_color = (0.85, 0.75, 0.60)

        return style

    def _fallback_tree_variations(
        self,
        trees: List[Dict],
        seed: Optional[int],
    ) -> Dict[str, PerObjectVariation]:
        """Generate deterministic tree variations without LLM."""
        import random
        rng = random.Random(seed or 42)

        variations = {}
        for i, tree in enumerate(trees):
            tree_id = str(tree.get("id", i))
            variations[tree_id] = PerObjectVariation(
                object_id=tree_id,
                hue_shift=rng.uniform(-10, 10),
                saturation_mult=rng.uniform(0.9, 1.1),
                brightness_mult=rng.uniform(0.92, 1.08),
            )
        return variations

    def _fallback_building_variations(
        self,
        buildings: List[Dict],
        seed: Optional[int],
    ) -> Dict[str, PerObjectVariation]:
        """Generate deterministic building variations without LLM."""
        import random
        rng = random.Random(seed or 42)

        variations = {}
        for i, bldg in enumerate(buildings):
            bldg_id = str(bldg.get("id", i))
            variations[bldg_id] = PerObjectVariation(
                object_id=bldg_id,
                hue_shift=rng.uniform(-5, 5),
                saturation_mult=rng.uniform(0.95, 1.05),
                brightness_mult=rng.uniform(0.96, 1.04),
            )
        return variations

    def _cache_key(self, content: str) -> str:
        """Generate cache key from content."""
        return hashlib.sha256(content.encode()).hexdigest()[:16]

    def _has_cache(self, key: str) -> bool:
        """Check if cache exists."""
        return (self.cache_dir / f"{key}.json").exists()

    def _load_cache(self, key: str) -> Any:
        """Load cached result."""
        with open(self.cache_dir / f"{key}.json") as f:
            return json.load(f)

    def _save_cache(self, key: str, data: Any) -> None:
        """Save result to cache."""
        with open(self.cache_dir / f"{key}.json", "w") as f:
            if hasattr(data, "to_dict"):
                json.dump(data.to_dict(), f, indent=2)
            elif isinstance(data, dict):
                # Handle PerObjectVariation dicts
                serializable = {}
                for k, v in data.items():
                    if hasattr(v, "__dict__"):
                        serializable[k] = v.__dict__
                    else:
                        serializable[k] = v
                json.dump(serializable, f, indent=2)
            else:
                json.dump(data, f, indent=2)


def generate_llm_style(
    prompt: str,
    seed: Optional[int] = None,
    use_cache: bool = True,
) -> LLMStyleOutput:
    """Convenience function to generate style from prompt.

    Args:
        prompt: Creative style description
        seed: Random seed for reproducibility
        use_cache: Whether to cache results

    Returns:
        Complete style parameters
    """
    generator = LLMVariationGenerator()
    return generator.generate_style(prompt, seed, use_cache)
